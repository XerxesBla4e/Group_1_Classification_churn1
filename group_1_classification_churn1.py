# -*- coding: utf-8 -*-
"""Group_1_Classification_churn1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jvrD2OWpimhWwrxWEEQLUEZfZeXnHHWg
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

import shap
import json
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score

from imblearn.over_sampling import SMOTE

from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

import kagglehub

path = kagglehub.dataset_download("blastchar/telco-customer-churn")

print("Path to dataset files:", path)

import os

files = os.listdir(path)
print("Files in the directory:", files)

file_path = "/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv"
df = pd.read_csv(file_path)


display(df.head(10))

display(df.info())

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')


display(df.isnull().sum())

df.dropna(subset=['TotalCharges'], inplace=True)

display(df.isnull().sum())

X = df.drop('Churn', axis=1)
y = df['Churn']
type(X)

y = y.apply(lambda x: 1 if x == 'Yes' else 0)

categorical_features = X.select_dtypes(include='object').columns.tolist()
print(categorical_features)
categorical_features.remove('customerID')
continuous_features = X.select_dtypes(include=np.number).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), continuous_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
     remainder='drop'
)

X_processed0 = preprocessor.fit_transform(X)

ohe_feature_names_clustering = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)
all_feature_names_clustering = continuous_features + list(ohe_feature_names_clustering)

X_processed = pd.DataFrame(X_processed0, columns=all_feature_names_clustering)

print("First 5 rows of the df_clustering DataFrame:")
print(X_processed.head())

display(df.describe())

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("whitegrid")

numerical_features = ['tenure', 'MonthlyCharges']
df[numerical_features].hist(bins=30, figsize=(15, 5))
plt.suptitle('Histograms of Numerical Features', y=1.02)
plt.tight_layout()
plt.show()

categorical_features = df.select_dtypes(include='object').columns.tolist()
categorical_features.remove('customerID')

fig, axes = plt.subplots(nrows=len(categorical_features)//3 + (len(categorical_features)%3 > 0), ncols=3, figsize=(15, 5 * (len(categorical_features)//3 + (len(categorical_features)%3 > 0))))
axes = axes.flatten()

for i, col in enumerate(categorical_features):
    sns.countplot(data=df, x=col, ax=axes[i], palette='viridis')
    axes[i].set_title(f'Distribution of {col}')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

df_corr = df.copy()
df_corr['Churn'] = y

numerical_df_corr = df_corr[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']]

correlation_matrix = numerical_df_corr.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Correlation Heatmap of Numerical Features and Churn')
plt.show()

df['TotalChargesPerTenure'] = df['TotalCharges'] / (df['tenure'] + 1e-6)
df['TotalChargesPerTenure'] = df['TotalChargesPerTenure'].replace([np.inf, -np.inf, np.nan], 0)


binary_columns = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity',
                  'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',
                  'StreamingMovies', 'PaperlessBilling']
for col in binary_columns:
    df[col] = df[col].apply(lambda x: 1 if x == 'Yes' else 0)

automatic_payment_methods = ['Bank transfer (automatic)', 'Credit card (automatic)']
df['PaymentMethod_Grouped'] = df['PaymentMethod'].apply(lambda x: 'Automatic' if x in automatic_payment_methods else 'Manual')

payment_method_grouped_dummies = pd.get_dummies(df['PaymentMethod_Grouped'], prefix='PaymentMethod_Grouped', drop_first=False)
df = pd.concat([df, payment_method_grouped_dummies], axis=1)

df.drop(['PaymentMethod', 'PaymentMethod_Grouped'], axis=1, inplace=True)


display(df.head())

from scipy import stats

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


if df['Churn'].dtype == 'object':
    df['Churn'] = df['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)

churned_monthly_charges = df[df['Churn'] == 1]['MonthlyCharges']
non_churned_monthly_charges = df[df['Churn'] == 0]['MonthlyCharges']


if len(churned_monthly_charges) > 1 and len(non_churned_monthly_charges) > 1:
    t_statistic, p_value = stats.ttest_ind(churned_monthly_charges, non_churned_monthly_charges)
    print(f"Independent Samples t-test for MonthlyCharges:")
    print(f"  t-statistic: {t_statistic:.4f}")
    print(f"  p-value: {p_value:.4f}")
else:
    print("Cannot perform t-test: Sample size for churned or non-churned customers is too small.")
    t_statistic, p_value = np.nan, np.nan

churned_df = df[df['Churn'] == 1].copy()
non_churned_df = df[df['Churn'] == 0].copy()

categorical_features_to_plot = ['Contract', 'PaymentMethod_Electronic check', 'InternetService', 'Partner', 'SeniorCitizen']
numerical_features_to_plot = ['tenure', 'MonthlyCharges', 'TotalCharges']

print("\nVisualizing Churn Differences for Categorical Features:")
for feature in categorical_features_to_plot:
    if feature in df.columns:
        plt.figure(figsize=(8, 5))
        sns.barplot(x=feature, y='Churn', data=df)
        plt.title(f'Churn Rate by {feature}')
        plt.ylabel('Churn Rate')
        plt.xlabel(feature)
        plt.show()
    else:
        print(f"Warning: Categorical feature '{feature}' not found in DataFrame.")


print("\nVisualizing Churn Differences for Numerical Features:")
for feature in numerical_features_to_plot:
     if feature in df.columns:
        plt.figure(figsize=(8, 5))
        sns.boxplot(x='Churn', y=feature, data=df)
        plt.title(f'{feature} Distribution by Churn')
        plt.xlabel('Churn (0: No, 1: Yes)')
        plt.ylabel(feature)
        plt.xticks([0, 1], ['No Churn', 'Churn'])
        plt.show()
     else:
        print(f"Warning: Numerical feature '{feature}' not found in DataFrame.")

churn_distribution = df['Churn'].value_counts()
print("Distribution of Churn (0: No Churn, 1: Churn):")
print(churn_distribution)

churn_percentage = df['Churn'].value_counts(normalize=True) * 100
print("\nPercentage Distribution of Churn:")
display(churn_percentage)

from sklearn.model_selection import train_test_split, cross_validate
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
import numpy as np
import pandas as pd

X = df.drop(['Churn', 'customerID'], axis=1)
y = df['Churn']

if y.dtype == 'object':
    y = y.apply(lambda x: 1 if x == 'Yes' else 0)


categorical_features = X.select_dtypes(include='object').columns.tolist()
continuous_features = X.select_dtypes(include=np.number).columns.tolist()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), continuous_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ],
    remainder='passthrough'
)

X_processed = preprocessor.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_resampled, y_train_resampled);

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_resampled, y_train_resampled);

gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train_resampled, y_train_resampled);

print("Models trained successfully using SMOTE-resampled data.")

"""### Ensemble Modeling

Ensemble methods combine multiple machine learning models to obtain better predictive performance than could be obtained from any of the constituent models alone. Here, we will use a **Soft Voting Classifier** to combine the predictions of the Logistic Regression, Random Forest, and Gradient Boosting models.

Soft voting averages the predicted probabilities for each class from the individual models, and then predicts the class with the highest average probability. This often helps to reduce variance and bias, leading to improved generalization.
"""

from sklearn.ensemble import VotingClassifier

# Soft Voting (averaging probabilities)
# Create a list of (name, estimator) tuples for the VotingClassifier
estimators = [
    ('lr', lr_model),
    ('rf', rf_model),
    ('gb', gb_model)
]

voting_clf = VotingClassifier(estimators=estimators, voting='soft')
voting_clf.fit(X_train_resampled, y_train_resampled) # Fit the voting classifier on resampled data

# Make predictions with the ensemble model
voting_test_pred = voting_clf.predict(X_test)
voting_test_proba = voting_clf.predict_proba(X_test)[:, 1]

# Evaluate the ensemble model
voting_accuracy = accuracy_score(y_test, voting_test_pred)
voting_precision = precision_score(y_test, voting_test_pred)
voting_recall = recall_score(y_test, voting_test_pred)
voting_f1 = f1_score(y_test, voting_test_pred)
voting_roc_auc = roc_auc_score(y_test, voting_test_proba)
voting_avg_precision = average_precision_score(y_test, voting_test_proba)

print("Ensemble (Soft Voting) Model Metrics:")
print(f"  Accuracy: {voting_accuracy:.4f}")
print(f"  Precision: {voting_precision:.4f}")
print(f"  Recall: {voting_recall:.4f}")
print(f"  F1 Score: {voting_f1:.4f}")
print(f"  ROC AUC: {voting_roc_auc:.4f}")
print(f"  Average Precision: {voting_avg_precision:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_curve, auc, precision_recall_curve

sns.set_style("whitegrid")

plt.figure(figsize=(10, 8))
plt.plot([0, 1], [0, 1], 'k--', label='Random')

# Individual models
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_test_proba)
roc_auc_lr = auc(fpr_lr, tpr_lr)
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')

fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_test_proba)
roc_auc_rf = auc(fpr_rf, tpr_rf)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')

fpr_gb, tpr_gb, _ = roc_curve(y_test, gb_test_proba)
roc_auc_gb = auc(fpr_gb, tpr_gb)
plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {roc_auc_gb:.2f})')

# Ensemble model
fpr_voting, tpr_voting, _ = roc_curve(y_test, voting_test_proba)
roc_auc_voting = auc(fpr_voting, tpr_voting)
plt.plot(fpr_voting, tpr_voting, label=f'Ensemble (Soft Voting) (AUC = {roc_auc_voting:.2f})', linewidth=2, linestyle='-')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves: Individual Models vs. Ensemble')
plt.legend(loc='lower right')
plt.show()

plt.figure(figsize=(10, 8))

# Individual models
precision_lr, recall_lr, _ = precision_recall_curve(y_test, lr_test_proba)
plt.plot(recall_lr, precision_lr, label=f'Logistic Regression (Avg Precision = {lr_avg_precision:.2f})')

precision_rf, recall_rf, _ = precision_recall_curve(y_test, rf_test_proba)
plt.plot(recall_rf, precision_rf, label=f'Random Forest (Avg Precision = {rf_avg_precision:.2f})')

precision_gb, recall_gb, _ = precision_recall_curve(y_test, gb_test_proba)
plt.plot(recall_gb, precision_gb, label=f'Gradient Boosting (Avg Precision = {gb_avg_precision:.2f})')

# Ensemble model
precision_voting, recall_voting, _ = precision_recall_curve(y_test, voting_test_proba)
plt.plot(recall_voting, precision_voting, label=f'Ensemble (Soft Voting) (Avg Precision = {voting_avg_precision:.2f})', linewidth=2, linestyle='-')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves: Individual Models vs. Ensemble')
plt.legend(loc='lower left')
plt.ylim([0, 1.05])
plt.show()

scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

print("Performing cross-validation...")
lr_cv_results = cross_validate(lr_model, X_train, y_train, cv=5, scoring=scoring)
rf_cv_results = cross_validate(rf_model, X_train, y_train, cv=5, scoring=scoring)
gb_cv_results = cross_validate(gb_model, X_train, y_train, cv=5, scoring=scoring)
print("Cross-validation completed.")

print("\nEvaluating models on the test set...")
lr_test_pred = lr_model.predict(X_test)
rf_test_pred = rf_model.predict(X_test)
gb_test_pred = gb_model.predict(X_test)

lr_test_metrics = {
    'accuracy': accuracy_score(y_test, lr_test_pred),
    'precision': precision_score(y_test, lr_test_pred),
    'recall': recall_score(y_test, lr_test_pred),
    'f1': f1_score(y_test, lr_test_pred)
}

rf_test_metrics = {
    'accuracy': accuracy_score(y_test, rf_test_pred),
    'precision': precision_score(y_test, rf_test_pred),
    'recall': recall_score(y_test, rf_test_pred),
    'f1': f1_score(y_test, rf_test_pred)
}

gb_test_metrics = {
    'accuracy': accuracy_score(y_test, gb_test_pred),
    'precision': precision_score(y_test, gb_test_pred),
    'recall': recall_score(y_test, gb_test_pred),
    'f1': f1_score(y_test, gb_test_pred)
}
print("Test set evaluation completed.")

print("\nCross-validation Results (Mean ± Std):")
print("Logistic Regression:")
for metric in scoring:
    if f'test_{metric}' in lr_cv_results:
        mean = lr_cv_results[f'test_{metric}'].mean()
        std = lr_cv_results[f'test_{metric}'].std()
        print(f"  {metric}: {mean:.4f} ± {std:.4f}")
    else:
        print(f"  {metric}: Metric not available in CV results")


print("\nRandom Forest:")
for metric in scoring:
    if f'test_{metric}' in rf_cv_results:
        mean = rf_cv_results[f'test_{metric}'].mean()
        std = rf_cv_results[f'test_{metric}'].std()
        print(f"  {metric}: {mean:.4f} ± {std:.4f}")
    else:
        print(f"  {metric}: N/A (Metric not available in CV results)")

print("\nGradient Boosting:")
for metric in scoring:
     if f'test_{metric}' in gb_cv_results:
        mean = gb_cv_results[f'test_{metric}'].mean()
        std = gb_cv_results[f'test_{metric}'].std()
        print(f"  {metric}: {mean:.4f} ± {std:.4f}")
     else:
        print(f"  {metric}: N/A (Metric not available in CV results)")


print("\nTest Set Evaluation Metrics:")
print("Logistic Regression:", lr_test_metrics)
print("Random Forest:", rf_test_metrics)
print("Gradient Boosting:", gb_test_metrics)

lr_test_proba = lr_model.predict_proba(X_test)[:, 1]
rf_test_proba = rf_model.predict_proba(X_test)[:, 1]
gb_test_proba = gb_model.predict_proba(X_test)[:, 1]

lr_roc_auc = roc_auc_score(y_test, lr_test_proba)
rf_roc_auc = roc_auc_score(y_test, rf_test_proba)
gb_roc_auc = roc_auc_score(y_test, gb_test_proba)

lr_avg_precision = average_precision_score(y_test, lr_test_proba)
rf_avg_precision = average_precision_score(y_test, rf_test_proba)
gb_avg_precision = average_precision_score(y_test, gb_test_proba)

print("\nTest Set ROC AUC and Average Precision:")
print(f"Logistic Regression - ROC AUC: {lr_roc_auc:.4f}, Average Precision: {lr_avg_precision:.4f}")
print(f"Random Forest - ROC AUC: {rf_roc_auc:.4f}, Average Precision: {rf_avg_precision:.4f}")
print(f"Gradient Boosting - ROC AUC: {gb_roc_auc:.4f}, Average Precision: {gb_avg_precision:.4f}")

print("\nPredicted probabilities for the positive class on the test set have been stored.")